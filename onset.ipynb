{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-27T21:19:31.238849Z",
     "start_time": "2024-05-27T21:19:31.213514Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils as utils\n",
    "import importlib\n",
    "import numpy as np\n",
    "import mir_eval\n",
    "import data\n",
    "import pickle\n",
    "\n",
    "import torch as th\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(data)\n",
    "\n",
    "train_dataset_path = './data/onset/train'\n",
    "test_dataset_path = './data/onset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Model descrbied in the paper plus droput\n",
    "class OnsetDetectionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OnsetDetectionCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=(3, 7))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(3, 1))\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=(3, 3))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(3, 1))\n",
    "        self.fc1 = nn.Linear(20 * 7 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 20 * 7 * 8) \n",
    "        x = self.dropout(F.relu(self.fc1(x)))  # Apply dropout\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = OnsetDetectionCNN()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T21:19:47.637077Z",
     "start_time": "2024-05-27T21:19:47.614867Z"
    }
   },
   "id": "8570ec556b0338dc"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class AudioOnsetDataset(th.utils.data.Dataset):\n",
    "    def __init__(self, spectograms, sample_rates, targets, sample_onsets, zero_sample_ratio=1.0):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "\n",
    "        for X, sample_rate, y, onsets in zip(spectograms, sample_rates, targets, sample_onsets):\n",
    "            # At this point we have a bunch of data samples which has labels 0 and small amount of labels 1 (0 - non-onsets, 1 - onsets)\n",
    "            X_frames, y_frames = utils.make_frames(X, y, onsets, sample_rate)\n",
    "            self.X += X_frames\n",
    "            self.y += y_frames\n",
    "\n",
    "        tmp = th.cat(self.X)\n",
    "        self.mean = th.mean(tmp, dim=(0, 2)).unsqueeze(1)\n",
    "        self.std = th.std(tmp, dim=(0, 2)).unsqueeze(1)\n",
    "        del tmp\n",
    "\n",
    "        self.X = [(x - self.mean)/self.std for x in self.X]\n",
    "        \n",
    "        # Here we balance out the samples so we will have equal amount of labels 1 and label 0\n",
    "        self._balance_dataset(zero_sample_ratio)\n",
    "\n",
    "    def _balance_dataset(self, zero_sample_ratio):\n",
    "        self.X = th.stack(self.X)\n",
    "        self.y = th.tensor(self.y)\n",
    "\n",
    "        pos_indices = (self.y == 1).nonzero(as_tuple=True)[0]\n",
    "        neg_indices = (self.y == 0).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        num_pos_samples = len(pos_indices)\n",
    "        num_neg_samples = int(num_pos_samples * zero_sample_ratio)\n",
    "        sampled_neg_indices = np.random.choice(neg_indices.cpu().numpy(), num_neg_samples, replace=False)\n",
    "\n",
    "        # Combine positive and sampled negative indices\n",
    "        balanced_indices = th.cat((pos_indices, th.tensor(sampled_neg_indices, dtype=th.long)))\n",
    "\n",
    "        # Shuffle the indices\n",
    "        balanced_indices = balanced_indices[th.randperm(len(balanced_indices))]\n",
    "\n",
    "        # Update X and y with balanced samples\n",
    "        self.X = self.X[balanced_indices]\n",
    "        self.y = self.y[balanced_indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T11:27:54.313249Z",
     "start_time": "2024-05-25T11:27:54.309395Z"
    }
   },
   "id": "91c5f3c8a3d2a3d7"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Load the paths and then split them into train and test set (validation set in our case for now).\n",
    "wav_files_paths_train, _, onset_files_paths_train, _ = utils.load_dataset_paths(train_dataset_path, is_train_dataset=True)\n",
    "X_train_paths, X_test_paths, y_train_paths, y_test_paths = train_test_split(wav_files_paths_train, onset_files_paths_train, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T11:40:09.853311Z",
     "start_time": "2024-05-25T11:40:09.845905Z"
    }
   },
   "id": "babe4c3fdc741e23"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:03<00:00, 27.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare train data\n",
    "features_train, sample_rates_train = utils.preprocess_audio(X_train_paths)\n",
    "onsets_train = utils.load_onsets(y_train_paths)\n",
    "labels_train = [utils.make_target(onsets_train[i], features_train[i].shape[-1], sample_rates_train[i]) for i in range(len(onsets_train))]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T11:40:14.109827Z",
     "start_time": "2024-05-25T11:40:10.431663Z"
    }
   },
   "id": "2a27f1087e84a38c"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:01<00:00, 22.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data (validation data in our case for now)\n",
    "features_test, sample_rates_test = utils.preprocess_audio(X_test_paths)\n",
    "onsets_test = utils.load_onsets(y_test_paths)\n",
    "labels_test = [utils.make_target(onsets_test[i], features_test[i].shape[-1], sample_rates_test[i]) for i in range(len(onsets_test))]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T11:40:15.280337Z",
     "start_time": "2024-05-25T11:40:14.103876Z"
    }
   },
   "id": "47ab781263be9c00"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "dataset = AudioOnsetDataset(features_train, sample_rates_train, labels_train, onsets_train)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# We save the mean and std so we can use it to normalize the validation data and the actual test data.\n",
    "mean_std = {'mean': dataset.mean, 'std': dataset.std}\n",
    "with open('mean_std.pkl', 'wb') as f:\n",
    "    pickle.dump(mean_std, f)\n",
    "\n",
    "# Save using torch\n",
    "th.save({'mean': dataset.mean, 'std': dataset.std}, 'mean_std.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T11:40:21.632767Z",
     "start_time": "2024-05-25T11:40:15.413251Z"
    }
   },
   "id": "97f460bd73892738"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if th.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T11:40:21.636415Z",
     "start_time": "2024-05-25T11:40:21.633799Z"
    }
   },
   "id": "83bc8d70621fc559"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def manual_evaluate(model, features, onsets, sample_rates, frame_size=15, thresholds=None, mean=None, std=None):\n",
    "    \n",
    "    # We can experience with different thresholds but during a lot of testing we found that 0.95 is a good chocie.\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.1, 1.0, 0.05)  # Define a range of thresholds to test\n",
    "        \n",
    "    if mean is None or std is None:\n",
    "        raise ValueError(\"Mean and std must be provided for normalization.\")\n",
    "\n",
    "    mean = mean.to(device)\n",
    "    std = std.to(device)\n",
    "    best_threshold = 0.0\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    half_frame_size = frame_size // 2\n",
    "\n",
    "    with th.no_grad():\n",
    "        for threshold in thresholds:\n",
    "            all_f1_scores = []\n",
    "            for i in range(len(features)):\n",
    "                predictions = []\n",
    "                \n",
    "                # Prepare features\n",
    "                f = features[i].to(device)\n",
    "                feature = (f - mean) / std  # Normalize the feature\n",
    "                \n",
    "                label = onsets[i]\n",
    "                num_frames = feature.shape[2]\n",
    "                \n",
    "                # Loop through on the frames\n",
    "                for j in range(half_frame_size, num_frames - half_frame_size):\n",
    "                    start = j - half_frame_size\n",
    "                    end = j + half_frame_size + 1\n",
    "                    \n",
    "                    input_frame = feature[:, :, start:end].unsqueeze(0).float()  # Add batch dimension\n",
    "                    output = model(input_frame).squeeze().item()\n",
    "                    predictions.append(output)\n",
    "                \n",
    "                # Smoothing the predictions the 10 hamming window is coming from trial and error.\n",
    "                predictions = np.convolve(predictions, np.hamming(10))\n",
    "                \n",
    "                res = []\n",
    "                for idx in find_peaks(predictions)[0]:\n",
    "                    if predictions[idx] >= threshold:\n",
    "                        res.append(idx * utils.HOP_LENGTH / utils.SAMPLING_RATE)\n",
    "                        \n",
    "                f, _, _ = mir_eval.onset.f_measure(label, np.array(res), window=0.05)  # 70 ms window\n",
    "                all_f1_scores.append(f)\n",
    "                \n",
    "            avg_f1 = np.mean(all_f1_scores)\n",
    "            if avg_f1 > best_f1:\n",
    "                best_f1 = avg_f1\n",
    "                best_threshold = threshold\n",
    "\n",
    "    return best_threshold, avg_f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T11:40:21.645079Z",
     "start_time": "2024-05-25T11:40:21.635613Z"
    }
   },
   "id": "6d49e1e12ddc8181"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.3390\n",
      "New best F1 score: 0.7082, model saved.\n",
      "TEST best_f1: 0.7082399294741945 with best threshold: 0.95\n",
      "Epoch [2/100], Loss: 0.3249\n",
      "TEST best_f1: 0.6961354523099221 with best threshold: 0.95\n",
      "Epoch [3/100], Loss: 0.3160\n",
      "New best F1 score: 0.7191, model saved.\n",
      "TEST best_f1: 0.7191097497023254 with best threshold: 0.95\n",
      "Epoch [4/100], Loss: 0.3086\n",
      "TEST best_f1: 0.7008580153248823 with best threshold: 0.95\n",
      "Epoch [5/100], Loss: 0.3033\n",
      "New best F1 score: 0.7393, model saved.\n",
      "TEST best_f1: 0.7393444251276311 with best threshold: 0.95\n",
      "Epoch [6/100], Loss: 0.2967\n",
      "TEST best_f1: 0.7156176678973748 with best threshold: 0.95\n",
      "Epoch [7/100], Loss: 0.2924\n",
      "TEST best_f1: 0.7148454343132985 with best threshold: 0.95\n",
      "Epoch [8/100], Loss: 0.2875\n",
      "TEST best_f1: 0.7167531645088647 with best threshold: 0.95\n",
      "Epoch [9/100], Loss: 0.2822\n",
      "TEST best_f1: 0.7348458908595803 with best threshold: 0.95\n",
      "Epoch [10/100], Loss: 0.2779\n",
      "TEST best_f1: 0.7385983020265746 with best threshold: 0.95\n",
      "Epoch [11/100], Loss: 0.2738\n",
      "New best F1 score: 0.7401, model saved.\n",
      "TEST best_f1: 0.7401455986927625 with best threshold: 0.95\n",
      "Epoch [12/100], Loss: 0.2695\n",
      "TEST best_f1: 0.7226436600711021 with best threshold: 0.95\n",
      "Epoch [13/100], Loss: 0.2658\n",
      "TEST best_f1: 0.7270846767532411 with best threshold: 0.95\n",
      "Epoch [14/100], Loss: 0.2610\n",
      "New best F1 score: 0.7515, model saved.\n",
      "TEST best_f1: 0.7515322380801711 with best threshold: 0.95\n",
      "Epoch [15/100], Loss: 0.2591\n",
      "TEST best_f1: 0.7418020569504987 with best threshold: 0.95\n",
      "Epoch [16/100], Loss: 0.2543\n",
      "TEST best_f1: 0.7501323992348473 with best threshold: 0.95\n",
      "Epoch [17/100], Loss: 0.2522\n",
      "New best F1 score: 0.7560, model saved.\n",
      "TEST best_f1: 0.7560403132379966 with best threshold: 0.95\n",
      "Epoch [18/100], Loss: 0.2500\n",
      "TEST best_f1: 0.737633844110696 with best threshold: 0.95\n",
      "Epoch [19/100], Loss: 0.2458\n",
      "TEST best_f1: 0.7433926180689064 with best threshold: 0.95\n",
      "Epoch [20/100], Loss: 0.2423\n",
      "TEST best_f1: 0.7458888936776014 with best threshold: 0.95\n",
      "Epoch [21/100], Loss: 0.2394\n",
      "TEST best_f1: 0.7423296599933112 with best threshold: 0.95\n",
      "Epoch [22/100], Loss: 0.2368\n",
      "New best F1 score: 0.7613, model saved.\n",
      "TEST best_f1: 0.7613401032083845 with best threshold: 0.95\n",
      "Epoch [23/100], Loss: 0.2338\n",
      "TEST best_f1: 0.7590786154331105 with best threshold: 0.95\n",
      "Epoch [24/100], Loss: 0.2321\n",
      "TEST best_f1: 0.7407068469064626 with best threshold: 0.95\n",
      "Epoch [25/100], Loss: 0.2290\n",
      "TEST best_f1: 0.7385418604375942 with best threshold: 0.95\n",
      "Epoch [26/100], Loss: 0.2273\n",
      "TEST best_f1: 0.7383129482887173 with best threshold: 0.95\n",
      "Epoch [27/100], Loss: 0.2244\n",
      "TEST best_f1: 0.7557084276922895 with best threshold: 0.95\n",
      "Epoch [28/100], Loss: 0.2217\n",
      "TEST best_f1: 0.7465149877490058 with best threshold: 0.95\n",
      "Epoch [29/100], Loss: 0.2193\n",
      "TEST best_f1: 0.7556239224592196 with best threshold: 0.95\n",
      "Epoch [30/100], Loss: 0.2167\n",
      "TEST best_f1: 0.7606472220229912 with best threshold: 0.95\n",
      "Epoch [31/100], Loss: 0.2154\n",
      "TEST best_f1: 0.7321393007914726 with best threshold: 0.95\n",
      "Epoch [32/100], Loss: 0.2123\n",
      "TEST best_f1: 0.7479730935236539 with best threshold: 0.95\n",
      "Epoch [33/100], Loss: 0.2096\n",
      "TEST best_f1: 0.752973513710627 with best threshold: 0.95\n",
      "Epoch [34/100], Loss: 0.2072\n",
      "TEST best_f1: 0.7536068223372866 with best threshold: 0.95\n",
      "Epoch [35/100], Loss: 0.2053\n",
      "New best F1 score: 0.7695, model saved.\n",
      "TEST best_f1: 0.7695118903483866 with best threshold: 0.95\n",
      "Epoch [36/100], Loss: 0.2023\n",
      "TEST best_f1: 0.7501877189997411 with best threshold: 0.95\n",
      "Epoch [37/100], Loss: 0.2022\n",
      "TEST best_f1: 0.7544684161622731 with best threshold: 0.95\n",
      "Epoch [38/100], Loss: 0.1978\n",
      "TEST best_f1: 0.7643106483196684 with best threshold: 0.95\n",
      "Epoch [39/100], Loss: 0.1966\n",
      "TEST best_f1: 0.7407813347339521 with best threshold: 0.95\n",
      "Epoch [40/100], Loss: 0.1939\n",
      "New best F1 score: 0.7714, model saved.\n",
      "TEST best_f1: 0.7713704083100817 with best threshold: 0.95\n",
      "Epoch [41/100], Loss: 0.1916\n",
      "TEST best_f1: 0.7613550571587548 with best threshold: 0.95\n",
      "Epoch [42/100], Loss: 0.1895\n",
      "TEST best_f1: 0.7701631090541853 with best threshold: 0.95\n",
      "Epoch [43/100], Loss: 0.1854\n",
      "TEST best_f1: 0.7585122769640698 with best threshold: 0.95\n",
      "Epoch [44/100], Loss: 0.1856\n",
      "TEST best_f1: 0.741842733892798 with best threshold: 0.95\n",
      "Epoch [45/100], Loss: 0.1843\n",
      "TEST best_f1: 0.7444449107668384 with best threshold: 0.95\n",
      "Epoch [46/100], Loss: 0.1810\n",
      "TEST best_f1: 0.75690005146083 with best threshold: 0.95\n",
      "Epoch [47/100], Loss: 0.1796\n",
      "TEST best_f1: 0.7558159866258656 with best threshold: 0.95\n",
      "Epoch [48/100], Loss: 0.1790\n",
      "TEST best_f1: 0.7578209249376621 with best threshold: 0.95\n",
      "Epoch [49/100], Loss: 0.1754\n",
      "TEST best_f1: 0.7647606181689508 with best threshold: 0.95\n",
      "Epoch [50/100], Loss: 0.1728\n",
      "TEST best_f1: 0.7576051284079548 with best threshold: 0.95\n",
      "Epoch [51/100], Loss: 0.1715\n",
      "TEST best_f1: 0.7410674180732681 with best threshold: 0.95\n",
      "Epoch [52/100], Loss: 0.1686\n",
      "TEST best_f1: 0.7547304667607371 with best threshold: 0.95\n",
      "Epoch [53/100], Loss: 0.1673\n",
      "TEST best_f1: 0.7664308540363017 with best threshold: 0.95\n",
      "Epoch [54/100], Loss: 0.1661\n",
      "TEST best_f1: 0.7449904945018281 with best threshold: 0.95\n",
      "Epoch [55/100], Loss: 0.1631\n",
      "TEST best_f1: 0.761914077965427 with best threshold: 0.95\n",
      "Epoch [56/100], Loss: 0.1617\n",
      "TEST best_f1: 0.7632094368893944 with best threshold: 0.95\n",
      "Epoch [57/100], Loss: 0.1590\n",
      "TEST best_f1: 0.7460839619540105 with best threshold: 0.95\n",
      "Epoch [58/100], Loss: 0.1579\n",
      "New best F1 score: 0.7719, model saved.\n",
      "TEST best_f1: 0.7719257439331952 with best threshold: 0.95\n",
      "Epoch [59/100], Loss: 0.1553\n",
      "TEST best_f1: 0.7621747050749099 with best threshold: 0.95\n",
      "Epoch [60/100], Loss: 0.1556\n",
      "TEST best_f1: 0.746128292811506 with best threshold: 0.95\n",
      "Epoch [61/100], Loss: 0.1514\n",
      "TEST best_f1: 0.7552874482622939 with best threshold: 0.95\n",
      "Epoch [62/100], Loss: 0.1498\n",
      "New best F1 score: 0.7751, model saved.\n",
      "TEST best_f1: 0.7751011112484368 with best threshold: 0.95\n",
      "Epoch [63/100], Loss: 0.1481\n",
      "TEST best_f1: 0.7670777270190394 with best threshold: 0.95\n",
      "Epoch [64/100], Loss: 0.1458\n",
      "TEST best_f1: 0.7674458188655321 with best threshold: 0.95\n",
      "Epoch [65/100], Loss: 0.1453\n",
      "TEST best_f1: 0.755770323486997 with best threshold: 0.95\n",
      "Epoch [66/100], Loss: 0.1434\n",
      "TEST best_f1: 0.7618791252454788 with best threshold: 0.95\n",
      "Epoch [67/100], Loss: 0.1421\n",
      "TEST best_f1: 0.7726234465290314 with best threshold: 0.95\n",
      "Epoch [68/100], Loss: 0.1400\n",
      "TEST best_f1: 0.7694199707841405 with best threshold: 0.95\n",
      "Epoch [69/100], Loss: 0.1375\n",
      "TEST best_f1: 0.7607827990538797 with best threshold: 0.95\n",
      "Epoch [70/100], Loss: 0.1358\n",
      "TEST best_f1: 0.7699960414858057 with best threshold: 0.95\n",
      "Epoch [71/100], Loss: 0.1338\n",
      "TEST best_f1: 0.7707791452230107 with best threshold: 0.95\n",
      "Epoch [72/100], Loss: 0.1329\n",
      "TEST best_f1: 0.7465005915308609 with best threshold: 0.95\n",
      "Epoch [73/100], Loss: 0.1294\n",
      "TEST best_f1: 0.748472283474523 with best threshold: 0.95\n",
      "Epoch [74/100], Loss: 0.1307\n",
      "TEST best_f1: 0.7660431047072537 with best threshold: 0.95\n",
      "Epoch [75/100], Loss: 0.1288\n",
      "TEST best_f1: 0.766471093926592 with best threshold: 0.95\n",
      "Epoch [76/100], Loss: 0.1259\n",
      "TEST best_f1: 0.7407567413199347 with best threshold: 0.95\n",
      "Epoch [77/100], Loss: 0.1250\n",
      "TEST best_f1: 0.7533853792818991 with best threshold: 0.95\n",
      "Epoch [78/100], Loss: 0.1224\n",
      "TEST best_f1: 0.7639314341661557 with best threshold: 0.95\n",
      "Epoch [79/100], Loss: 0.1218\n",
      "TEST best_f1: 0.7684698593018406 with best threshold: 0.95\n",
      "Epoch [80/100], Loss: 0.1199\n",
      "TEST best_f1: 0.7536089176362888 with best threshold: 0.95\n",
      "Epoch [81/100], Loss: 0.1179\n",
      "TEST best_f1: 0.749959628407584 with best threshold: 0.95\n",
      "Epoch [82/100], Loss: 0.1162\n",
      "TEST best_f1: 0.7637697715371863 with best threshold: 0.95\n",
      "Epoch [83/100], Loss: 0.1138\n",
      "TEST best_f1: 0.7721221254723024 with best threshold: 0.95\n",
      "Epoch [84/100], Loss: 0.1135\n",
      "TEST best_f1: 0.7646557282169014 with best threshold: 0.95\n",
      "Epoch [85/100], Loss: 0.1133\n",
      "TEST best_f1: 0.7577928259446208 with best threshold: 0.95\n",
      "Epoch [86/100], Loss: 0.1101\n",
      "TEST best_f1: 0.7653174288615718 with best threshold: 0.95\n",
      "Epoch [87/100], Loss: 0.1100\n",
      "TEST best_f1: 0.7519096702839038 with best threshold: 0.95\n",
      "Epoch [88/100], Loss: 0.1093\n",
      "TEST best_f1: 0.7641289988635401 with best threshold: 0.95\n",
      "Epoch [89/100], Loss: 0.1056\n",
      "TEST best_f1: 0.7566390359440334 with best threshold: 0.95\n",
      "Epoch [90/100], Loss: 0.1046\n",
      "TEST best_f1: 0.7612157784880826 with best threshold: 0.95\n",
      "Epoch [91/100], Loss: 0.1022\n",
      "TEST best_f1: 0.7559240145539046 with best threshold: 0.95\n",
      "Epoch [92/100], Loss: 0.1022\n",
      "TEST best_f1: 0.7648692452317504 with best threshold: 0.95\n",
      "Epoch [93/100], Loss: 0.1020\n",
      "TEST best_f1: 0.7597555496317387 with best threshold: 0.95\n",
      "Epoch [94/100], Loss: 0.0999\n",
      "TEST best_f1: 0.7591023111849468 with best threshold: 0.95\n",
      "Epoch [95/100], Loss: 0.0969\n",
      "TEST best_f1: 0.7622285100003907 with best threshold: 0.95\n",
      "Epoch [96/100], Loss: 0.0969\n",
      "TEST best_f1: 0.7580852852690178 with best threshold: 0.95\n",
      "Epoch [97/100], Loss: 0.0953\n",
      "TEST best_f1: 0.7683544745694648 with best threshold: 0.95\n",
      "Epoch [98/100], Loss: 0.0937\n",
      "TEST best_f1: 0.7593683337526809 with best threshold: 0.95\n",
      "Epoch [99/100], Loss: 0.0929\n",
      "TEST best_f1: 0.7619743720138027 with best threshold: 0.95\n",
      "Epoch [100/100], Loss: 0.0939\n",
      "TEST best_f1: 0.7621725804089299 with best threshold: 0.95\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 3e-4\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "\n",
    "best_f1_test = 0.0  # Initialize best F1 score\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        labels = labels.float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    best_treshold, current_f1_test = manual_evaluate(model, features_test, onsets_test, sample_rates_test, thresholds=[0.95], mean=dataset.mean, std=dataset.std)\n",
    "\n",
    "    # Check if the current F1 score is the best we have seen so far\n",
    "    if current_f1_test > best_f1_test:\n",
    "        best_f1_test = current_f1_test\n",
    "        # Save the model\n",
    "        th.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"New best F1 score: {best_f1_test:.4f}, model saved.\")\n",
    "\n",
    "    print(f\"TEST best_f1: {current_f1_test} with best threshold: {best_treshold}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T12:56:42.982174Z",
     "start_time": "2024-05-25T11:44:26.588991Z"
    }
   },
   "id": "3b6c7cef1a1c4997"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-25T11:43:45.212536Z"
    }
   },
   "id": "d311e11b76fecb82"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
